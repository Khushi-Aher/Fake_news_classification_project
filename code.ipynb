{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\khush\\AppData\\Roaming\\nltk_data...\n",
      "c:\\Users\\khush\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:07:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Results:\n",
      "Accuracy: 0.9891133422039434\n",
      "Precision: 0.9850485920757538\n",
      "Recall: 0.9924679889530504\n",
      "F1 Score: 0.988744372186093\n",
      "AUC-ROC: 0.9994809874740758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Preprocessing Functions\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stopwords.words('english')]\n",
    "    return ' '.join(text)\n",
    "\n",
    "# Apply Preprocessing\n",
    "data_train['cleaned_title'] = data_train['title'].apply(clean_text)\n",
    "data_train['cleaned_text'] = data_train['text'].apply(clean_text)\n",
    "data_test['cleaned_title'] = data_test['title'].apply(clean_text)\n",
    "data_test['cleaned_text'] = data_test['text'].apply(clean_text)\n",
    "\n",
    "# Additional Feature Engineering\n",
    "def feature_engineering(data):\n",
    "    data['title_length'] = data['title'].apply(lambda x: len(str(x).split()))\n",
    "    data['text_length'] = data['text'].apply(lambda x: len(str(x).split()))\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    data['title_sentiment'] = data['title'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n",
    "    data['text_sentiment'] = data['text'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n",
    "    data['title_word_density'] = data['title_length'] / (data['text_length'] + 1)  # Avoid division by zero\n",
    "    return data\n",
    "\n",
    "# Engineer Features\n",
    "data_train = feature_engineering(data_train)\n",
    "data_test = feature_engineering(data_test)\n",
    "\n",
    "# Vectorization\n",
    "tfidf_title = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))  # Include bigrams\n",
    "tfidf_text = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and Transform Train Data\n",
    "title_features_train = tfidf_title.fit_transform(data_train['cleaned_title'])\n",
    "text_features_train = tfidf_text.fit_transform(data_train['cleaned_text'])\n",
    "\n",
    "# Transform Test Data\n",
    "title_features_test = tfidf_title.transform(data_test['cleaned_title'])\n",
    "text_features_test = tfidf_text.transform(data_test['cleaned_text'])\n",
    "\n",
    "# Dimensionality Reduction\n",
    "svd_title = TruncatedSVD(n_components=300, random_state=42)  # Reduce to 300 dimensions\n",
    "svd_text = TruncatedSVD(n_components=300, random_state=42)\n",
    "\n",
    "title_features_train = svd_title.fit_transform(title_features_train)\n",
    "text_features_train = svd_text.fit_transform(text_features_train)\n",
    "title_features_test = svd_title.transform(title_features_test)\n",
    "text_features_test = svd_text.transform(text_features_test)\n",
    "\n",
    "# Combine Features for Train and Test\n",
    "X_train = np.hstack((title_features_train, text_features_train, \n",
    "                     data_train[['title_length', 'text_length', 'title_sentiment', \n",
    "                                 'text_sentiment', 'title_word_density']].values))\n",
    "X_test = np.hstack((title_features_test, text_features_test, \n",
    "                    data_test[['title_length', 'text_length', 'title_sentiment', \n",
    "                               'text_sentiment', 'title_word_density']].values))\n",
    "\n",
    "# Handle Class Imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Define Models with Hyperparameter Tuning\n",
    "param_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 6],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), \n",
    "                   param_xgb, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "param_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "rf = GridSearchCV(RandomForestClassifier(random_state=42), param_rf, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Train Models\n",
    "xgb.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Ensemble Model\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('XGBoost', xgb.best_estimator_),\n",
    "    ('Random Forest', rf.best_estimator_),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42))\n",
    "], voting='soft', n_jobs=-1)\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Models\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "y_proba = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "print(\"Voting Classifier Results:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"AUC-ROC:\", auc_roc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare Results for Output\n",
    "output = []\n",
    "for idx, (title, prediction) in enumerate(zip(data_test['title'], y_pred)):\n",
    "    output.append([title, int(prediction)])\n",
    "\n",
    "# Save Results\n",
    "with open('result.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"{\\n\")\n",
    "    for item in output:\n",
    "        f.write(f\"  {item},\\n\")\n",
    "    f.write(\"}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
